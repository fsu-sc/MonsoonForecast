{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging landmark 1\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n",
      "Debugging landmark 1.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "#Files\n",
    "data_dir = \"/Net/elnino/data/obs/ERA5/global/daily/\"\n",
    "# Example: '/Net/elnino/data/obs/ERA5/global/daily/mslp_era5_day_2002.nc'\n",
    "\n",
    "# List all files \n",
    "all_files = os.listdir(data_dir)\n",
    "\n",
    "#file_info_list = []\n",
    "file_info_dict = {}\n",
    "#file_info = pd.DataFrame(columns=[\"filename\", \"year\", \"variable\"])\n",
    "\n",
    "\n",
    "\n",
    "for filename in all_files:\n",
    "    #\n",
    "    parts = filename.split(\"_\")\n",
    "    year = int(parts[-1].split(\".\")[0])\n",
    "    variable = parts[0]\n",
    "    \n",
    "\n",
    "    #file_info_list.append((filename, year, variable))\n",
    "    # If the variable is not already a key in the dictionary, initialize an empty DataFrame\n",
    "    if variable not in file_info_dict:\n",
    "        file_info_dict[variable] = pd.DataFrame(columns=[\"filename\", \"year\"])\n",
    "    \n",
    "    # Append information to the DataFrame associated with the variable\n",
    "    #file_info_dict[variable] = file_info_dict[variable].append({\"filename\": filename, \"year\": year}, ignore_index=True)\n",
    "    new_df = pd.DataFrame({\"filename\": [filename], \"year\": [year]})\n",
    "    file_info_dict[variable] = pd.concat([file_info_dict[variable], new_df], ignore_index=True)\n",
    "\n",
    "#file_info = pd.DataFrame(file_info_list, columns=[\"filename\", \"year\", \"variable\"])\n",
    "# dictionary to store grouped dataArrays for each year\n",
    "data_by_year = {}\n",
    "\n",
    "print(\"Debugging landmark 1\")\n",
    "\n",
    "# Iterate over each distinct variable in file_info_dict\n",
    "for variable_name, variable_info in file_info_dict.items():\n",
    "    # Iterate over unique years for the current variable\n",
    "    for year in variable_info[\"year\"].unique():\n",
    "        # Filter files for the current year and the previous year\n",
    "        year_files = variable_info[(variable_info[\"year\"] == year) | (variable_info[\"year\"] == year - 1)][\"filename\"].tolist()\n",
    "    \n",
    "        # Initialize an empty list to store DataArrays for the current variable\n",
    "        variable_data = []\n",
    "        print(\"Debugging landmark 1.5\")\n",
    "  \n",
    "        for filename in year_files:\n",
    "            ds = xr.open_dataset(os.path.join(data_dir, filename))\n",
    "            variable_data.append(ds[variable_name])\n",
    "        \n",
    "        # Concatenate the DataArrays along a new dimension (e.g., variable)\n",
    "        stacked_data = xr.concat(variable_data, dim=\"variable\")\n",
    "    \n",
    "        # Store the stacked DataArray in the dictionary with the year as the key\n",
    "        if year in data_by_year:\n",
    "            data_by_year[year][variable_name] = stacked_data\n",
    "        else:\n",
    "            data_by_year[year] = {variable_name: stacked_data}\n",
    "\n",
    "print(\"Debugging landmark 2\")\n",
    "\n",
    "csv_path = \"/unity/f2/aoleksy/MonsoonForecast/onset_pen_FL.csv\"\n",
    "ground_truth = pd.read_csv(csv_path)\n",
    "\n",
    "for year, gt_value in zip(ground_truth[\"year\"], ground_truth[\"varToPredict\"]):\n",
    "    if year in data_by_year:\n",
    "        data_by_year[year][\"ground_truth\"] = gt_value\n",
    "\n",
    "print(\"Debugging landmark 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_year = {}\n",
    "\n",
    "print(\"Debugging landmark 1\")\n",
    "\n",
    "for year in file_info_dict[\"year\"].unique():\n",
    "   #current year and previous year so that we can predict onset date further back\n",
    "    year_files = file_info_dict[(file_info_dict[\"year\"] == year) | (file_info_dict[\"year\"] == year - 1)][\"filename\"].tolist()\n",
    "    \n",
    "\n",
    "    variable_data = []\n",
    "    print(\"Debugging landmark 1.5\")\n",
    "  \n",
    "    for filename in year_files:\n",
    "   \n",
    "        ds = xr.open_dataset(os.path.join(data_dir, filename))\n",
    "        \n",
    "       \n",
    "        variable_data.append(ds[variable])\n",
    "        \n",
    "    # ***WILL PROBABLY HAVE TO MODIFY THIS***\n",
    "    stacked_data = xr.concat(variable_data, dim=\"variable\")\n",
    "    \n",
    "    \n",
    "    data_by_year[year] = stacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds   \n",
    "data_dir = \"/Net/elnino/data/obs/ERA5/global/daily/\"\n",
    "year = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010]\n",
    "offsets =[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "# file_dic = ds.createFileInfoDict(data_dir)\n",
    "test_year = [1990,1991,1992,1993,1994,1995,1996,1997,1998,1999]\n",
    "train_data = ds.NetCDFDataset(data_dir,year,offsets)\n",
    "test_data = ds.NetCDFDataset(data_dir,test_year,offsets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166, 166, 166, 166, 166, 166, 166, 166, 166, 166, 166, 166, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 162, 162, 162, 162, 162, 162, 162, 162, 162, 162, 162, 162, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aieoastorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
